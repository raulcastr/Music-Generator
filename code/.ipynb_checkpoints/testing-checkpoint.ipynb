{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def read_batches(all_chars, unique_chars):\\n    length = all_chars.shape[0]\\n    batch_chars = int(lenght / BATCH_SIZE)\\n    \\n    for start in range(0, batch_chars - SEQ_LENGTH, 64):\\n        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))\\n        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))\\n        for batch_index in range (0, 16):\\n            for i in range(0,64):\\n                X[batch_index, i ] = all_chars[batch_index * batch_chars + start + i]\\n                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1\\n        yield X, Y\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def read_batches(all_chars, unique_chars):\n",
    "    length = all_chars.shape[0]\n",
    "    batch_chars = int(lenght / BATCH_SIZE)\n",
    "    \n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, 64):\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))\n",
    "        for batch_index in range (0, 16):\n",
    "            for i in range(0,64):\n",
    "                X[batch_index, i ] = all_chars[batch_index * batch_chars + start + i]\n",
    "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1\n",
    "        yield X, Y\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s define a function straight away for reading the MIDI files. It returns the array of notes and chords present in the musical file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to read MIDI files\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will load the MIDI files into our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_midi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bcc40ce062bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#reading each midi file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnotes_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mread_midi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-bcc40ce062bd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#reading each midi file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnotes_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mread_midi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'read_midi' is not defined"
     ]
    }
   ],
   "source": [
    "#for listing down the file names\n",
    "import os\n",
    "\n",
    "\n",
    "#specify the path\n",
    "path='../data/mozart_beeth/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under this section, we will explore the dataset and understand it in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see here, no. of unique notes is 304. Now, let us see the distribution of the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([277.,  30.,   6.,   8.,   4.,   6.,   9.,   5.,   2.,   2.]),\n",
       " array([1.0000e+00, 5.8570e+02, 1.1704e+03, 1.7551e+03, 2.3398e+03,\n",
       "        2.9245e+03, 3.5092e+03, 4.0939e+03, 4.6786e+03, 5.2633e+03,\n",
       "        5.8480e+03]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 326
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "\n",
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#library for visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above plot, we can infer that most of the notes have a very low frequency. So, let us keep the top frequent notes and ignore the low-frequency ones. Here, I am defining the threshold as 50. Nevertheless, the parameter can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see here, no. of frequently occurring notes is around 170.  Now, let us prepare new musical files which contain only the top frequent notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the input and output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will assign a unique integer to every note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will prepare the integer sequences for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, prepare the integer sequences for output data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us preserve 80% of the data for training and the rest 20% for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### I have defined 2 architectures here – WaveNet and LSTM. Please experiment with both the architectures to understand the importance of WaveNet architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm():\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(128,return_sequences=True))\n",
    "  model.add(LSTM(128))\n",
    "  model.add(Dense(256))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(n_vocab))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have simplified the architecture of the WaveNet without adding residual and skip connections since the role of these layers is to improve the faster convergence (and WaveNet takes raw audio wave as input). But in our case, the input would be a set of nodes and chords since we are generating music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           19100     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 191)               49087     \n",
      "=================================================================\n",
      "Total params: 276,507\n",
      "Trainable params: 276,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the callback to save the best model during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s train the model with a batch size of 128 for 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 4.0104\n",
      "Epoch 00001: val_loss improved from inf to 3.84892, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 43s 39ms/step - loss: 4.0104 - val_loss: 3.8489\n",
      "Epoch 2/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.6497\n",
      "Epoch 00002: val_loss improved from 3.84892 to 3.67321, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 43s 39ms/step - loss: 3.6497 - val_loss: 3.6732\n",
      "Epoch 3/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.5246\n",
      "Epoch 00003: val_loss improved from 3.67321 to 3.57924, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.5246 - val_loss: 3.5792\n",
      "Epoch 4/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.4461\n",
      "Epoch 00004: val_loss improved from 3.57924 to 3.51720, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.4459 - val_loss: 3.5172\n",
      "Epoch 5/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.3814\n",
      "Epoch 00005: val_loss improved from 3.51720 to 3.49380, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.3814 - val_loss: 3.4938\n",
      "Epoch 6/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.3326\n",
      "Epoch 00006: val_loss improved from 3.49380 to 3.41767, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.3328 - val_loss: 3.4177\n",
      "Epoch 7/50\n",
      "1112/1114 [============================>.] - ETA: 0s - loss: 3.2921\n",
      "Epoch 00007: val_loss did not improve from 3.41767\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.2924 - val_loss: 3.4186\n",
      "Epoch 8/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.2583\n",
      "Epoch 00008: val_loss improved from 3.41767 to 3.35522, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.2583 - val_loss: 3.3552\n",
      "Epoch 9/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.2251\n",
      "Epoch 00009: val_loss improved from 3.35522 to 3.32906, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.2252 - val_loss: 3.3291\n",
      "Epoch 10/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.1964\n",
      "Epoch 00010: val_loss improved from 3.32906 to 3.30925, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.1964 - val_loss: 3.3093\n",
      "Epoch 11/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.1734\n",
      "Epoch 00011: val_loss improved from 3.30925 to 3.29510, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 41s 37ms/step - loss: 3.1734 - val_loss: 3.2951\n",
      "Epoch 12/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.1507\n",
      "Epoch 00012: val_loss improved from 3.29510 to 3.27440, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 41s 37ms/step - loss: 3.1507 - val_loss: 3.2744\n",
      "Epoch 13/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 3.1331\n",
      "Epoch 00013: val_loss improved from 3.27440 to 3.26272, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 43s 39ms/step - loss: 3.1331 - val_loss: 3.2627\n",
      "Epoch 14/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.1141\n",
      "Epoch 00014: val_loss improved from 3.26272 to 3.23668, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.1141 - val_loss: 3.2367\n",
      "Epoch 15/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.1025\n",
      "Epoch 00015: val_loss improved from 3.23668 to 3.22175, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.1025 - val_loss: 3.2217\n",
      "Epoch 16/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.0859\n",
      "Epoch 00016: val_loss improved from 3.22175 to 3.20428, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.0859 - val_loss: 3.2043\n",
      "Epoch 17/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.0734\n",
      "Epoch 00017: val_loss did not improve from 3.20428\n",
      "1114/1114 [==============================] - 41s 37ms/step - loss: 3.0736 - val_loss: 3.2263\n",
      "Epoch 18/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 3.0627\n",
      "Epoch 00018: val_loss improved from 3.20428 to 3.20045, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.0627 - val_loss: 3.2005\n",
      "Epoch 19/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.0493\n",
      "Epoch 00019: val_loss improved from 3.20045 to 3.20030, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 3.0493 - val_loss: 3.2003\n",
      "Epoch 20/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 3.0420\n",
      "Epoch 00020: val_loss improved from 3.20030 to 3.19072, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 43s 38ms/step - loss: 3.0420 - val_loss: 3.1907\n",
      "Epoch 21/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.0309\n",
      "Epoch 00021: val_loss improved from 3.19072 to 3.17148, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 44s 39ms/step - loss: 3.0311 - val_loss: 3.1715\n",
      "Epoch 22/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 3.0221\n",
      "Epoch 00022: val_loss did not improve from 3.17148\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.0221 - val_loss: 3.1863\n",
      "Epoch 23/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.0155\n",
      "Epoch 00023: val_loss did not improve from 3.17148\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.0155 - val_loss: 3.1757\n",
      "Epoch 24/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.0082\n",
      "Epoch 00024: val_loss improved from 3.17148 to 3.16401, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.0082 - val_loss: 3.1640\n",
      "Epoch 25/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 3.0007\n",
      "Epoch 00025: val_loss improved from 3.16401 to 3.15326, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 3.0008 - val_loss: 3.1533\n",
      "Epoch 26/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9945\n",
      "Epoch 00026: val_loss improved from 3.15326 to 3.15207, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 43s 39ms/step - loss: 2.9945 - val_loss: 3.1521\n",
      "Epoch 27/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 2.9867\n",
      "Epoch 00027: val_loss improved from 3.15207 to 3.15160, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 43s 38ms/step - loss: 2.9867 - val_loss: 3.1516\n",
      "Epoch 28/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9798- ETA: 0s - loss\n",
      "Epoch 00028: val_loss improved from 3.15160 to 3.13569, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9798 - val_loss: 3.1357\n",
      "Epoch 29/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9744\n",
      "Epoch 00029: val_loss did not improve from 3.13569\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9746 - val_loss: 3.1370\n",
      "Epoch 30/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9715\n",
      "Epoch 00030: val_loss did not improve from 3.13569\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9716 - val_loss: 3.1473\n",
      "Epoch 31/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 2.9625\n",
      "Epoch 00031: val_loss improved from 3.13569 to 3.12343, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 37ms/step - loss: 2.9625 - val_loss: 3.1234\n",
      "Epoch 32/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9586\n",
      "Epoch 00032: val_loss did not improve from 3.12343\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9586 - val_loss: 3.1286\n",
      "Epoch 33/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9549\n",
      "Epoch 00033: val_loss did not improve from 3.12343\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9549 - val_loss: 3.1325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9459\n",
      "Epoch 00034: val_loss improved from 3.12343 to 3.12161, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9460 - val_loss: 3.1216\n",
      "Epoch 35/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9464\n",
      "Epoch 00035: val_loss did not improve from 3.12161\n",
      "1114/1114 [==============================] - 43s 39ms/step - loss: 2.9463 - val_loss: 3.1269\n",
      "Epoch 36/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 2.9418\n",
      "Epoch 00036: val_loss improved from 3.12161 to 3.11510, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9418 - val_loss: 3.1151\n",
      "Epoch 37/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 2.9359\n",
      "Epoch 00037: val_loss did not improve from 3.11510\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9359 - val_loss: 3.1191\n",
      "Epoch 38/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9345\n",
      "Epoch 00038: val_loss did not improve from 3.11510\n",
      "1114/1114 [==============================] - 42s 38ms/step - loss: 2.9345 - val_loss: 3.1160\n",
      "Epoch 39/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9273\n",
      "Epoch 00039: val_loss did not improve from 3.11510\n",
      "1114/1114 [==============================] - 43s 38ms/step - loss: 2.9273 - val_loss: 3.1180\n",
      "Epoch 40/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9265\n",
      "Epoch 00040: val_loss did not improve from 3.11510\n",
      "1114/1114 [==============================] - 43s 38ms/step - loss: 2.9266 - val_loss: 3.1168\n",
      "Epoch 41/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9188\n",
      "Epoch 00041: val_loss improved from 3.11510 to 3.10222, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 47s 42ms/step - loss: 2.9187 - val_loss: 3.1022\n",
      "Epoch 42/50\n",
      "1114/1114 [==============================] - ETA: 0s - loss: 2.9183\n",
      "Epoch 00042: val_loss did not improve from 3.10222\n",
      "1114/1114 [==============================] - 45s 41ms/step - loss: 2.9183 - val_loss: 3.1183\n",
      "Epoch 43/50\n",
      "1112/1114 [============================>.] - ETA: 0s - loss: 2.9166\n",
      "Epoch 00043: val_loss improved from 3.10222 to 3.10031, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 37s 33ms/step - loss: 2.9167 - val_loss: 3.1003\n",
      "Epoch 44/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9146\n",
      "Epoch 00044: val_loss improved from 3.10031 to 3.09938, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 38s 34ms/step - loss: 2.9145 - val_loss: 3.0994\n",
      "Epoch 45/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9103\n",
      "Epoch 00045: val_loss did not improve from 3.09938\n",
      "1114/1114 [==============================] - 37s 33ms/step - loss: 2.9104 - val_loss: 3.0995\n",
      "Epoch 46/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9070\n",
      "Epoch 00046: val_loss did not improve from 3.09938\n",
      "1114/1114 [==============================] - 38s 34ms/step - loss: 2.9071 - val_loss: 3.1002\n",
      "Epoch 47/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9010\n",
      "Epoch 00047: val_loss did not improve from 3.09938\n",
      "1114/1114 [==============================] - 38s 34ms/step - loss: 2.9010 - val_loss: 3.1019\n",
      "Epoch 48/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.9022\n",
      "Epoch 00048: val_loss improved from 3.09938 to 3.09390, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 38s 34ms/step - loss: 2.9023 - val_loss: 3.0939\n",
      "Epoch 49/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.8965- ETA - ETA: 1\n",
      "Epoch 00049: val_loss improved from 3.09390 to 3.08060, saving model to best_model.h5\n",
      "1114/1114 [==============================] - 38s 34ms/step - loss: 2.8965 - val_loss: 3.0806\n",
      "Epoch 50/50\n",
      "1113/1114 [============================>.] - ETA: 0s - loss: 2.8969-  - ETA: 0s - loss: 2\n",
      "Epoch 00050: val_loss did not improve from 3.08060\n",
      "1114/1114 [==============================] - 38s 34ms/step - loss: 2.8969 - val_loss: 3.0947\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Its time to compose our own music now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-025a989e15e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrandom_music\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions=[]\n",
    "for i in range(30):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "    \n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will convert the integers back into the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final step is to convert back the predictions into a MIDI file. Let’s define the function to accomplish the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the predictions into a musical file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
